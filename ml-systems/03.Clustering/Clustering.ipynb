{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Clustering\n",
    "\n",
    "unsupervised/text\n",
    "\n",
    "## Problem of posts similarity\n",
    "\n",
    "**Bag-of-word** approach - simple word counts as its basis.\n",
    "For each word its occurence is counted and noted in a vector \n",
    "(*vectorization*)\n",
    "\n",
    "### Clustering steps\n",
    "\n",
    "1. Extract the salient features from each post and store it as vector per post\n",
    "2. Compute clustering on the vectors\n",
    "3. Determine the cluster for the post in question\n",
    "4. From this cluster, fetch a handful of posts that are different from the post in question.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "min_df determines how CountVectorizer treats words not used frequently\n",
    "\n",
    "if integer - less than that value will be dropped\n",
    "if fraction -less than fraction of the overall dataset will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\",\n",
    "           \"Hard disk format problems\"]\n",
    "\n",
    "X = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Imaging databases provide storage capabilities.', 'This is a toy post about machine learning. Actually, it contains not much interesting stuff.', 'Imaging databases store data. Imaging databases store data. Imaging databases store data.', 'Most imaging databases save images permanently.\\n', 'Imaging databases store data.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DIR = os.path.join(os.getcwd(), \"data\")\n",
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\n",
    "\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec)\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive similarity measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(dist_lambda):\n",
    "    best_doc = None\n",
    "    best_dist = 999999999\n",
    "    best_i = None\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(posts)\n",
    "    new_post_vec = vectorizer.transform([new_post])\n",
    "    for i in range(0, num_samples):\n",
    "        post = posts[i]\n",
    "\n",
    "        if post==new_post:\n",
    "            continue\n",
    "        post_vec = X_train.getrow(i)\n",
    "        d = dist_lambda(post_vec, new_post_vec)\n",
    "        print(\"Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 0 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "Post 1 with dist=3.16: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "Post 2 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Post 3 with dist=1.73: Most imaging databases save images permanently.\n",
      "\n",
      "Post 4 with dist=1.41: Imaging databases store data.\n",
      "Best post is 4 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "calculate_similarity(dist_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n",
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(2).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 0 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "Post 1 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "Post 2 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Post 3 with dist=0.92: Most imaging databases save images permanently.\n",
      "\n",
      "Post 4 with dist=0.77: Imaging databases store data.\n",
      "Best post is 2 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "calculate_similarity(dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing less important words\n",
    "\n",
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorizer.get_stop_words())[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "We count 'imaging' and 'images' as different words. **Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'graphic', u'imag', u'imag')\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "\n",
    "# For english, we can take SnowballStemmer\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "print(\n",
    "    s.stem(\"graphics\"),\n",
    "    s.stem(\"imaging\"),\n",
    "    s.stem(\"image\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the vectorizer with NLTK's stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'actual',\n",
       " u'capabl',\n",
       " u'contain',\n",
       " u'data',\n",
       " u'databas',\n",
       " u'imag',\n",
       " u'interest',\n",
       " u'learn',\n",
       " u'machin',\n",
       " u'perman',\n",
       " u'post',\n",
       " u'provid',\n",
       " u'save',\n",
       " u'storag',\n",
       " u'store',\n",
       " u'stuff',\n",
       " u'toy']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "        \n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "vectorizer.fit_transform(posts)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post 0 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "Post 1 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "Post 2 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Post 3 with dist=0.63: Most imaging databases save images permanently.\n",
      "\n",
      "Post 4 with dist=0.77: Imaging databases store data.\n",
      "Best post is 3 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "calculate_similarity(dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words on steroids\n",
    "\n",
    "We wantterm that occurs often in particular post and very rarely anywhere else.\n",
    "This is **term frequency - inverse document frequency (TF-IDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def tfidf(term, doc, docset):\n",
    "    tf = float(doc.count(term)/sum(doc.count(w) for w in docset))\n",
    "    idf = math.log(float(len(docset))/(len([doc for doc in docset if term in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (\n",
    "            english_stemmer.stem(w) for w in analzyer(doc)\n",
    "        )\n",
    "    \n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be aware:\n",
    "- not captuing relations\n",
    "- no negations (solution: unigrams)\n",
    "- misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
